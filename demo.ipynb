{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_tUTD6M8kP6HP_wZFe7k9YbZvGkCnVJ6",
      "authorship_tag": "ABX9TyNPwemF/DIuoh+yhpgOuNoP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Levos06/AlmostGPT/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHpWT4_gupji",
        "outputId": "f7f0691f-5f8b-4bf9-daa9-d25329fbd180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AlmostGPT'...\n",
            "remote: Enumerating objects: 83, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 83 (delta 32), reused 71 (delta 21), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (83/83), 36.17 KiB | 2.01 MiB/s, done.\n",
            "Resolving deltas: 100% (32/32), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Levos06/AlmostGPT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd AlmostGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDs1MTHUuwrM",
        "outputId": "e995ecef-6ce6-4b64-c208-4d66857c9d38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AlmostGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets==2.18.0 fsspec==2024.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJCww17dvutm",
        "outputId": "a51e66aa-89d9-4094-e170-cb3b1a72db97"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==2.18.0\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting fsspec==2024.2.0\n",
            "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.18.0)\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->datasets==2.18.0) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->datasets==2.18.0) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.18.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.18.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.18.0) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0) (1.17.0)\n",
            "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: pyarrow-hotfix, fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.2.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.18.0 fsspec-2024.2.0 pyarrow-hotfix-0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPPILdgNvZrf",
        "outputId": "5632cc89-a891-4423-d492-62bcca705cc3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 5.9868\n",
            "Модель сохранена по пути: /content/drive/MyDrive/my_transformer.pth\n",
            "Epoch 2, Loss: 5.2384\n",
            "Epoch 3, Loss: 4.9851\n",
            "Epoch 4, Loss: 4.8296\n",
            "Epoch 5, Loss: 4.7241\n",
            "Epoch 6, Loss: 4.5607\n",
            "Epoch 7, Loss: 4.3785\n",
            "Epoch 8, Loss: 4.2046\n",
            "Epoch 9, Loss: 3.9805\n",
            "Epoch 10, Loss: 3.7912\n",
            "Epoch 11, Loss: 3.5693\n",
            "Модель сохранена по пути: /content/drive/MyDrive/my_transformer.pth\n",
            "Epoch 12, Loss: 3.4322\n",
            "Epoch 13, Loss: 3.2475\n",
            "Epoch 14, Loss: 3.0476\n",
            "Epoch 15, Loss: 2.8866\n",
            "Epoch 16, Loss: 2.7135\n",
            "Epoch 17, Loss: 2.5370\n",
            "Epoch 18, Loss: 2.3926\n",
            "Epoch 19, Loss: 2.2308\n",
            "Epoch 20, Loss: 2.0696\n",
            "Модель сохранена по пути: /content/drive/MyDrive/my_transformer.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformer import Transformer\n",
        "from tokenizer import BPETokenizer\n",
        "from utils import generate_src_mask, generate_tgt_mask\n",
        "\n",
        "# Путь к сохранённой модели\n",
        "PATH = '/content/drive/MyDrive/my_transformer.pth'\n",
        "\n",
        "# Параметры модели\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "num_layers = 6\n",
        "dropout = 0.1\n",
        "max_len = 128\n",
        "src_vocab_size = 219\n",
        "tgt_vocab_size = 343\n",
        "\n",
        "# Устройство\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Загрузка модели\n",
        "model = Transformer(\n",
        "    d_model=d_model,\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    max_len=max_len,\n",
        "    d_ff=d_ff,\n",
        "    num_heads=num_heads,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(PATH, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Токенизаторы\n",
        "tokenizer_src = BPETokenizer()\n",
        "tokenizer_src.load_vocab(\"vocab_en.json\")\n",
        "tokenizer_tgt = BPETokenizer()\n",
        "tokenizer_tgt.load_vocab(\"vocab_de.json\")\n",
        "\n",
        "sos_token = 1\n",
        "eos_token = 2\n",
        "\n",
        "def create_causal_mask(size):\n",
        "    # Нижнетреугольная матрица [size x size] для tgt\n",
        "    return torch.tril(torch.ones((size, size), dtype=torch.bool)).to(device)\n",
        "\n",
        "def translate_sentence(sentence, max_len=40):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src_tokens = tokenizer_src.encode(sentence)\n",
        "        print(src_tokens)\n",
        "        src_tensor = torch.tensor(src_tokens, dtype=torch.long).unsqueeze(0).to(device)  # [1, src_len]\n",
        "\n",
        "        src_mask = torch.ones((1, 1, len(src_tokens), len(src_tokens)), dtype=torch.bool).to(device)\n",
        "\n",
        "        tgt_input = torch.tensor([[sos_token]], dtype=torch.long).to(device)\n",
        "        output_tokens = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            tgt_mask = create_causal_mask(tgt_input.size(1)).unsqueeze(0).unsqueeze(1)  # [1, 1, tgt_len, tgt_len]\n",
        "            memory_mask = torch.ones((1, 1, tgt_input.size(1), src_tensor.size(1)), dtype=torch.bool).to(device)\n",
        "\n",
        "            logits = model(\n",
        "                src_tensor,\n",
        "                tgt_input,\n",
        "                src_mask=src_mask,\n",
        "                tgt_mask=tgt_mask,\n",
        "                memory_mask=memory_mask\n",
        "            )\n",
        "            #print(logits)\n",
        "\n",
        "            next_token_id = logits.argmax(-1)[:, -1].item()\n",
        "            if next_token_id == eos_token:\n",
        "                break\n",
        "            output_tokens.append(next_token_id)\n",
        "            tgt_input = torch.cat([tgt_input, torch.tensor([[next_token_id]], device=device)], dim=1)\n",
        "\n",
        "        return tokenizer_tgt.decode(output_tokens)\n",
        "\n",
        "# Проверка\n",
        "sentences = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"This is a test sentence.\",\n",
        "    \"Translate this example.\"\n",
        "]\n",
        "\n",
        "for sent in sentences:\n",
        "    print(f\"Input: {sent}\")\n",
        "    print(f\"Output: {translate_sentence(sent)}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtRQtyQfxA73",
        "outputId": "d8faf9cd-0db6-4e36-eb15-0d02558e140f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Hello, how are you?\n",
            "[38, 185, 69, 72, 162, 179, 80, 156, 82, 102, 30]\n",
            "Output: Ein Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in \n",
            "\n",
            "Input: This is a test sentence.\n",
            "[50, 200, 116, 58, 4, 77, 186, 99, 147, 71, 77, 157, 60, 62, 16]\n",
            "Output: Ein Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in \n",
            "\n",
            "Input: Translate this example.\n",
            "[50, 149, 71, 76, 109, 188, 92, 116, 62, 81, 58, 70, 73, 126, 16]\n",
            "Output: Ein Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in Mann in \n",
            "\n"
          ]
        }
      ]
    }
  ]
}